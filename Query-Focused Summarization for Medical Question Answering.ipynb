{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9pWsPAt0dCh"
   },
   "source": [
    "# Query-Focused Summarization for Medical Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. **Model 1: Simple Siamese Neural Network**\n",
    "   - Defining the Model Architecture\n",
    "   - Custom Distance Layer Implementation\n",
    "   - Training the Model\n",
    "   - Evaluating the Model\n",
    "   - Reporting F1 Scores\n",
    "\n",
    "2. **Model 2: Recurrent Neural Network with LSTM**\n",
    "   - Embedding Layer Design\n",
    "   - LSTM Layer and Hidden Layers Setup\n",
    "   - Model Training and Evaluation\n",
    "   - Performance Comparison with Simple Siamese NN\n",
    "   - Reporting F1 Scores\n",
    "\n",
    "3. **Model 3: Transformer-based Neural Network**\n",
    "   - BERT Feature Extraction\n",
    "   - Transformer Encoder Layers\n",
    "   - Model Architecture and Hidden Layers\n",
    "   - Model Training and Evaluation\n",
    "   - Reporting F1 Scores\n",
    "   - Summarization Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wL4Tek5m1ZlU"
   },
   "source": [
    "# Data Review\n",
    "\n",
    "The following code uses pandas to store the file `bioasq10_labelled.csv` in a data frame and show the first rows of data. For this code to run, first you need to unzip the file `data.zip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1726,
     "status": "ok",
     "timestamp": 1716012512166,
     "user": {
      "displayName": "Jasmine Huynh",
      "userId": "16972213468483330608"
     },
     "user_tz": -600
    },
    "id": "xa0cLT0t1Zld",
    "outputId": "f1f77617-e349-4824-9f78-39ee510b8e0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>sentid</th>\n",
       "      <th>question</th>\n",
       "      <th>sentence text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>Hirschsprung disease (HSCR) is a multifactoria...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>In this study, we review the identification of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>The majority of the identified genes are relat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>The non-Mendelian inheritance of sporadic non-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>Coding sequence mutations in e.g.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qid  sentid                                           question  \\\n",
       "0    0       0  Is Hirschsprung disease a mendelian or a multi...   \n",
       "1    0       1  Is Hirschsprung disease a mendelian or a multi...   \n",
       "2    0       2  Is Hirschsprung disease a mendelian or a multi...   \n",
       "3    0       3  Is Hirschsprung disease a mendelian or a multi...   \n",
       "4    0       4  Is Hirschsprung disease a mendelian or a multi...   \n",
       "\n",
       "                                       sentence text  label  \n",
       "0  Hirschsprung disease (HSCR) is a multifactoria...      0  \n",
       "1  In this study, we review the identification of...      1  \n",
       "2  The majority of the identified genes are relat...      1  \n",
       "3  The non-Mendelian inheritance of sporadic non-...      1  \n",
       "4                  Coding sequence mutations in e.g.      0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"bioasq10b_labelled.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lolcz2b_1Zlh"
   },
   "source": [
    "The columns of the CSV file are:\n",
    "\n",
    "* `qid`: an ID for a question. Several rows may have the same question ID, as we can see above.\n",
    "* `sentid`: an ID for a sentence.\n",
    "* `question`: The text of the question. In the above example, the first rows all have the same question: \"Is Hirschsprung disease a mendelian or a multifactorial disorder?\"\n",
    "* `sentence text`: The text of the sentence.\n",
    "* `label`: 1 if the sentence is a part of the answer, 0 if the sentence is not part of the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VTTgRnN0dC4"
   },
   "source": [
    "# Task 1: Simple Siamese NN\n",
    "\n",
    "Implement a simple TensorFlow-Keras neural model that has the following sequence of layers:\n",
    "\n",
    "1. An input layer that will accept the tf.idf of triplet data. The input of Siamese network is a triplet, consisting of anchor (i.e., the question), positive answer, negative answer.\n",
    "2. 3 hidden layers and a relu activation function. You need to determine the size of the hidden layers.\n",
    "3. Implement a class that serves as a distance layer. It returns the squared Euclidean distance between anchor and positive answer, as well as that between anchor and negative answer\n",
    "4. Implement a function that prepares raw data in csv files into triplets. Note that it is important to keep the similar number of positive pairs and negative pairs. For example, if a question has 10 anwsers, then we at most can have 10 positive pairs and it is good to associate this question with 10~20 negative sentences.\n",
    "\n",
    "\n",
    "Train the model with the training data and use the `dev_test` set to determine a good size of the hidden layer.\n",
    "\n",
    "With the model that you have trained, implement a summariser that returns the $n$ sentences with highest predicted score. Use the following function signature:\n",
    "\n",
    "```{python}\n",
    "def nn_summariser(csvfile, questionids, n=1):\n",
    "   \"\"\"Return the IDs of the n sentences that have the highest predicted score.\n",
    "      The input questionids is a list of question ids.\n",
    "      The output is a list of lists of sentence ids\n",
    "   \"\"\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Import necessary libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Layer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import metrics, optimizers, callbacks\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Set Random Seeds and Define Constants:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "random.seed(1234)\n",
    "\n",
    "# Constants\n",
    "MAX_LEN = 10000  # Large enough to capture sufficient information, but needs to be adjusted based on available memory.\n",
    "EPOCHS = 9  # Balances training time and performance.\n",
    "BATCH_SIZE = 40  # Smaller batch size for memory efficiency, suitable for machines with limited RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Generate random triplets from CSV file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_triplets(csv_file):\n",
    "    \"\"\"\n",
    "    Generate random triplets from the given CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    csv_file (str): Path to the CSV file containing the dataset.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Three lists containing anchor inputs, positive inputs, and negative inputs.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a DataFrame\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Separate the data into positive and negative samples based on the \"label\" column\n",
    "    positive_data = data[data[\"label\"] == 1]  # Positive samples have label 1\n",
    "    negative_data = data[data[\"label\"] == 0]  # Negative samples have label 0\n",
    "\n",
    "    # Convert the \"question\" column of positive samples into a list of anchor inputs\n",
    "    a_input = np.array(positive_data[\"question\"].to_list())\n",
    "\n",
    "    # Convert the \"sentence text\" column of positive samples into a list of positive inputs\n",
    "    p_input = np.array(positive_data[\"sentence text\"].to_list())\n",
    "\n",
    "    # Convert the \"sentence text\" column of negative samples into a list of negative inputs\n",
    "    n_input = np.array(negative_data[\"sentence text\"].to_list())\n",
    "\n",
    "    # Shuffling positive samples to ensure randomness\n",
    "    random.seed(10)  # Set the seed for reproducibility\n",
    "    indices = list(range(len(a_input)))  # Create a list of indices for the positive samples\n",
    "    random.shuffle(indices)  # Shuffle the indices\n",
    "    a_input = a_input[indices].tolist()  # Reorder anchor inputs based on the shuffled indices\n",
    "    p_input = p_input[indices].tolist()  # Reorder positive inputs based on the shuffled indices\n",
    "\n",
    "    # Shuffling negative samples to ensure randomness\n",
    "    random.seed(1234)  # Set a different seed for reproducibility\n",
    "    indices = list(range(len(n_input)))  # Create a list of indices for the negative samples\n",
    "    random.shuffle(indices)  # Shuffle the indices\n",
    "    n_input = n_input[indices].tolist()  # Reorder negative inputs based on the shuffled indices\n",
    "\n",
    "    # Ensuring balanced dataset by limiting negative samples to the number of positive samples\n",
    "    return a_input, p_input, n_input[:len(a_input)]\n",
    "    \n",
    "# Positive and negative samples are shuffled to introduce randomness, which helps the model generalize better.\n",
    "# By limiting the number of negative samples to match the number of positive samples, we ensure a balanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Load and transform data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "a_input, p_input, n_input = generate_random_triplets('training.csv')\n",
    "a_input_test, p_input_test, n_input_test = generate_random_triplets('dev_test.csv')\n",
    "a_input_test2, p_input_test2, n_input_test2 = generate_random_triplets('test.csv')\n",
    "\n",
    "# Convert text data to tf-idf vectors\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", max_features=MAX_LEN)\n",
    "tfidf.fit(a_input + p_input + n_input)\n",
    "\n",
    "a_input = tfidf.transform(a_input).toarray()\n",
    "p_input = tfidf.transform(p_input).toarray()\n",
    "n_input = tfidf.transform(n_input).toarray()\n",
    "a_input_test = tfidf.transform(a_input_test).toarray()\n",
    "p_input_test = tfidf.transform(p_input_test).toarray()\n",
    "n_input_test = tfidf.transform(n_input_test).toarray()\n",
    "a_input_test2 = tfidf.transform(a_input_test2).toarray()\n",
    "p_input_test2 = tfidf.transform(p_input_test2).toarray()\n",
    "n_input_test2 = tfidf.transform(n_input_test2).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Define custom distance layer:**\n",
    "\n",
    "The layer takes three inputs (anchor, positive, and negative) and returns the distances between anchor-positive and anchor-negative pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDistanceLayer(Layer):\n",
    "    \"\"\"\n",
    "    Custom Keras layer to calculate the squared Euclidean distance between inputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)  # Calculate squared Euclidean distance between anchor and positive sample\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)  # Calculate squared Euclidean distance between anchor and negative sample\n",
    "        return ap_distance, an_distance  # Return both distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Define custom SiameseModel class:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from tensorflow.keras import metrics, optimizers\n",
    "\n",
    "class CustomSiameseModel(Model):\n",
    "    \"\"\"\n",
    "    Custom Keras Model class for a Siamese network.\n",
    "\n",
    "    Attributes:\n",
    "    siamese_network (Model): The Siamese neural network.\n",
    "    margin (float): Margin for the triplet loss function.\n",
    "    \"\"\"\n",
    "    def __init__(self, siamese_network, margin=1e-18):\n",
    "        super().__init__()\n",
    "        self.siamese_network = siamese_network  # Initialize with the Siamese neural network\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")  # Track the loss using Keras metrics\n",
    "        self.margin = margin  # Set the margin for the triplet loss function\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)  # Forward pass through the Siamese network\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        \"\"\"\n",
    "        Training step function.\n",
    "\n",
    "        Parameters:\n",
    "        data (tuple): Input data for training.\n",
    "\n",
    "        Returns:\n",
    "        dict: Loss value.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.triplet_loss(data)  # Calculate the triplet loss\n",
    "            trainable_vars = self.siamese_network.trainable_weights  # Get trainable weights\n",
    "            gradients = tape.gradient(loss, trainable_vars)  # Compute gradients\n",
    "            self.optimizer.apply_gradients(zip(gradients, trainable_vars))  # Apply gradients to update weights\n",
    "            self.loss_tracker.update_state(loss)  # Update loss tracker\n",
    "        return {\"loss\": self.loss_tracker.result()}  # Return the current loss\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        \"\"\"\n",
    "        Testing step function.\n",
    "\n",
    "        Parameters:\n",
    "        data (tuple): Input data for testing.\n",
    "\n",
    "        Returns:\n",
    "        dict: Loss value.\n",
    "        \"\"\"\n",
    "        loss = self.triplet_loss(data)  # Calculate the triplet loss\n",
    "        self.loss_tracker.update_state(loss)  # Update loss tracker\n",
    "        return {\"loss\": self.loss_tracker.result()}  # Return the current loss\n",
    "\n",
    "    @tf.function\n",
    "    def triplet_loss(self, data):\n",
    "        \"\"\"\n",
    "        Triplet loss function.\n",
    "\n",
    "        Parameters:\n",
    "        data (tuple): Input data containing anchor, positive, and negative samples.\n",
    "\n",
    "        Returns:\n",
    "        tensor: Calculated triplet loss.\n",
    "        \"\"\"\n",
    "        ap_distance, an_distance = self.siamese_network(data)  # Get distances from the Siamese network\n",
    "        loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)  # Calculate triplet loss with margin\n",
    "        return loss  # Return the loss value\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]  # Return the metrics to track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Define and test summarizer function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_summariser(csvfile, questionids, n=1):\n",
    "    \"\"\"\n",
    "    Return the IDs of the n sentences that have the highest predicted score.\n",
    "\n",
    "    Parameters:\n",
    "    csvfile (str): Path to the CSV file containing the dataset.\n",
    "    questionids (list): List of question IDs.\n",
    "    n (int): Number of top sentences to return.\n",
    "\n",
    "    Returns:\n",
    "    list: List of lists containing sentence IDs.\n",
    "    \"\"\"\n",
    "    test_data = pd.read_csv(csvfile)  # Load the dataset from the CSV file\n",
    "    total = []  # Initialize a list to store the results for each question ID\n",
    "\n",
    "    for qid in questionids:\n",
    "        # Extract anchor, positive, and negative samples for the given question ID\n",
    "        anchor_input = list(test_data.loc[(test_data[\"qid\"] == qid) & (test_data[\"sentid\"] == 0)][\"question\"])  # Extract question text\n",
    "        positive_input = list(test_data.loc[(test_data[\"qid\"] == qid) & (test_data[\"sentid\"] == 0)][\"sentence text\"])  # Extract positive sentence text\n",
    "        negative_input = list(test_data.loc[(test_data[\"qid\"] == qid)][\"sentence text\"])  # Extract all sentence texts for the question\n",
    "        \n",
    "        # Check if any input is empty and handle it\n",
    "        if not anchor_input or not positive_input or not negative_input:\n",
    "            print(f\"Skipping question ID {qid} due to empty inputs.\")\n",
    "            continue\n",
    "\n",
    "        # Repeat the positive and anchor inputs to match the number of negative inputs\n",
    "        positive_input *= len(negative_input)\n",
    "        anchor_input *= len(negative_input)\n",
    "\n",
    "        # Transform the text data into tf-idf vectors\n",
    "        a_input = tfidf.transform(anchor_input).toarray()  # Transform anchor input\n",
    "        p_input = tfidf.transform(positive_input).toarray()  # Transform positive input\n",
    "        n_input = tfidf.transform(negative_input).toarray()  # Transform negative input\n",
    "\n",
    "        # Get predictions from the Siamese model\n",
    "        ap_distance_test, an_distance_test = siamese_model.predict([a_input, p_input, n_input], batch_size=40)  # Predict distances\n",
    "        sorted_indices = list(np.argsort(an_distance_test))  # Sort the indices based on distance to get the most similar sentences\n",
    "        total.append(sorted_indices[:n])  # Append the top n sentence IDs to the total results\n",
    "\n",
    "    return total  # Return the list of lists containing sentence IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Question ID 6:\n",
      "Question: Which miRNAs could be used as potential biomarkers for epithelial ovarian cancer?\n",
      "Top sentence ID 2: Upregulation of microRNA-203 is associated with advanced tumor progression and poor prognosis in epithelial ovarian cancer\n",
      "\n",
      "Question ID 7:\n",
      "Question: Which acetylcholinesterase inhibitors are used for treatment of myasthenia gravis?\n",
      "Top sentence ID 13: Pyridostigmine has been used as a treatment for MG for over 50 years and is generally considered safe.\n",
      "\n",
      "Question ID 10:\n",
      "Question: Name synonym of Acrokeratosis paraneoplastica.\n",
      "Top sentence ID 21: Bazex syndrome: acrokeratosis paraneoplastica.\n",
      "\n",
      "Question ID 13:\n",
      "Question: Which are the major characteristics of cellular senescence?\n",
      "Top sentence ID 1: Its defining characteristics are arrested cell-cycle progression and the development of aberrant gene expression with proinflammatory behavior.\n",
      "\n",
      "Question ID 35:\n",
      "Question: What are the main indications of lacosamide?\n",
      "Top sentence ID 9: Apart from this, LCM has demonstrated potent effects in animal models for a variety of CNS disorders like schizophrenia and stress induced anxiety.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage with the test set (using the first five unique question IDs)\n",
    "data = pd.read_csv(\"test.csv\")\n",
    "test_question_ids = data['qid'].unique()[:5].tolist()  # Select the first five unique question IDs\n",
    "top_sentences = nn_summariser(\"test.csv\", test_question_ids, n=1)\n",
    "\n",
    "# Define a function to display the top sentences\n",
    "def display_top_sentences(question_ids, top_sent_ids, data):\n",
    "    for qid, sent_ids in zip(question_ids, top_sent_ids):\n",
    "        print(f\"Question ID {qid}:\")\n",
    "        question = data[data['qid'] == qid]['question'].iloc[0]\n",
    "        print(f\"Question: {question}\")\n",
    "        for sid in sent_ids:\n",
    "            sentence = data[(data['qid'] == qid) & (data['sentid'] == sid)]['sentence text'].iloc[0]\n",
    "            print(f\"Top sentence ID {sid}: {sentence}\")\n",
    "        print()\n",
    "\n",
    "display_top_sentences(test_question_ids, top_sentences, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Calculate labels for F1 score:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_labels_for_f1_score(csvfile, n):\n",
    "    \"\"\"\n",
    "    Calculate the predicted and true labels for F1 score calculation.\n",
    "\n",
    "    Parameters:\n",
    "    csvfile (str): Path to the CSV file containing the dataset.\n",
    "    n (int): Number of questions to process.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Lists of predicted and true labels.\n",
    "    \"\"\"\n",
    "    test_data = pd.read_csv(csvfile)  # Load the dataset from the CSV file\n",
    "    total_predict = []  # Initialize a list to store predicted labels\n",
    "    total_true = []  # Initialize a list to store true labels\n",
    "    for qid in list(test_data[\"qid\"].unique())[:n]:\n",
    "        # Get the predicted sentence IDs for the given question ID\n",
    "        predict_values = nn_summariser(csvfile, [qid], len(test_data.loc[(test_data[\"qid\"] == qid)]))[0]\n",
    "        predicted_0 = predict_values[-1]  # Get the ID of the sentence with the highest predicted score\n",
    "        predicted_1 = predict_values[0]  # Get the ID of the sentence with the lowest predicted score\n",
    "        \n",
    "        # Get the true labels for the predicted sentences\n",
    "        y_true = list(test_data.loc[(test_data[\"qid\"] == qid) & (test_data[\"sentid\"] == predicted_1)][\"label\"])[0]  # True label for the highest scored sentence\n",
    "        y_false = list(test_data.loc[(test_data[\"qid\"] == qid) & (test_data[\"sentid\"] == predicted_0)][\"label\"])[0]  # True label for the lowest scored sentence\n",
    "        \n",
    "        total_predict.extend([1, 0])  # Extend the predicted labels list with [1, 0]\n",
    "        total_true.extend([y_true, y_false])  # Extend the true labels list with the actual labels\n",
    "    return total_predict, total_true  # Return the lists of predicted and true labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. Evaluate different hidden layer configurations:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating configuration: (64, 32, 32)\n",
      "Epoch 1/12\n",
      "140/143 [============================>.] - ETA: 0s - loss: 1.1933e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 3s 19ms/step - loss: 1.1951e-04 - val_loss: 1.4165e-04\n",
      "Epoch 2/12\n",
      "141/143 [============================>.] - ETA: 0s - loss: 1.1957e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 2s 12ms/step - loss: 1.1951e-04 - val_loss: 1.4165e-04\n",
      "Epoch 3/12\n",
      "140/143 [============================>.] - ETA: 0s - loss: 1.1891e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 2s 13ms/step - loss: 1.1951e-04 - val_loss: 1.4165e-04\n",
      "Epoch 4/12\n",
      "140/143 [============================>.] - ETA: 0s - loss: 1.2015e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 2s 12ms/step - loss: 1.1951e-04 - val_loss: 1.4165e-04\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "F1 Score for (64, 32, 32): 0.4835164835164836\n",
      "Evaluating configuration: (256, 128, 128)\n",
      "Epoch 1/12\n",
      "142/143 [============================>.] - ETA: 0s - loss: 1.5852e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 1.5819e-04 - val_loss: 2.1405e-04\n",
      "Epoch 2/12\n",
      "142/143 [============================>.] - ETA: 0s - loss: 1.5816e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 7s 46ms/step - loss: 1.5819e-04 - val_loss: 2.1405e-04\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "F1 Score for (256, 128, 128): 0.46153846153846156\n",
      "Evaluating configuration: (128, 64, 64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "142/143 [============================>.] - ETA: 0s - loss: 1.4548e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 4s 24ms/step - loss: 1.4586e-04 - val_loss: 1.9820e-04\n",
      "Epoch 2/12\n",
      "142/143 [============================>.] - ETA: 0s - loss: 1.4578e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 1.4586e-04 - val_loss: 1.9820e-04\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "F1 Score for (128, 64, 64): 0.5333333333333332\n",
      "All configuration results:\n",
      "Configuration (64, 32, 32): F1 Score = 0.4835164835164836\n",
      "Configuration (256, 128, 128): F1 Score = 0.46153846153846156\n",
      "Configuration (128, 64, 64): F1 Score = 0.5333333333333332\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_configs = [\n",
    "    (64, 32, 32),  # Configuration 1: Three layers with 64, 32, and 32 units respectively\n",
    "    (256, 128, 128),  # Configuration 2: Three layers with 256, 128, and 128 units respectively\n",
    "    (128, 64, 64)  # Configuration 3: Three layers with 128, 64, and 64 units respectively\n",
    "]\n",
    "\n",
    "results = {}  # Dictionary to store F1 scores for each configuration\n",
    "\n",
    "for config in hidden_layer_configs:\n",
    "    print(f\"Evaluating configuration: {config}\")\n",
    "    # Define the input layers\n",
    "    first_sent_in_a = Input(shape=(MAX_LEN,))  # Input layer for anchor sentences\n",
    "    second_sent_in_p = Input(shape=(MAX_LEN,))  # Input layer for positive sentences\n",
    "    third_sent_in_n = Input(shape=(MAX_LEN,))  # Input layer for negative sentences\n",
    "\n",
    "    # Define the neural network model\n",
    "    model = Sequential([\n",
    "        Dense(config[0], activation='relu'),  # First hidden layer with ReLU activation\n",
    "        Dense(config[1], activation='relu'),  # Second hidden layer with ReLU activation\n",
    "        Dense(config[2], activation='relu')  # Third hidden layer with ReLU activation\n",
    "    ])\n",
    "\n",
    "    # Encode the inputs\n",
    "    encoded_1 = model(first_sent_in_a)  # Encode anchor sentences\n",
    "    encoded_2 = model(second_sent_in_p)  # Encode positive sentences\n",
    "    encoded_3 = model(third_sent_in_n)  # Encode negative sentences\n",
    "\n",
    "    # Create the Siamese network\n",
    "    loss_layer = CustomDistanceLayer()(encoded_1, encoded_2, encoded_3)  # Custom distance layer\n",
    "    siamese_network = Model([first_sent_in_a, second_sent_in_p, third_sent_in_n], loss_layer)  # Siamese network model\n",
    "\n",
    "    # Compile and train the model\n",
    "    siamese_model = CustomSiameseModel(siamese_network)  # Custom Siamese model\n",
    "    siamese_model.compile(optimizer=optimizers.Adam(learning_rate=5e-19))  # Compile with Adam optimizer\n",
    "    callback = callbacks.EarlyStopping(monitor='loss', patience=1)  # Early stopping callback\n",
    "\n",
    "    siamese_model.fit([a_input, p_input, n_input], epochs=12, batch_size=80,\n",
    "                      validation_data=([a_input_test, p_input_test, n_input_test]), callbacks=[callback])  # Train the model\n",
    "\n",
    "    # Calculate and print F1 score\n",
    "    total_predict, total_true = calculate_labels_for_f1_score(\"test.csv\", 50)  # Get predictions and true labels\n",
    "    f1 = f1_score(total_true, total_predict)  # Calculate F1 score\n",
    "    results[config] = f1  # Store the F1 score for the current configuration\n",
    "    print(f\"F1 Score for {config}: {f1}\")\n",
    "\n",
    "# Print all results\n",
    "print(\"All configuration results:\")\n",
    "for config, f1 in results.items():\n",
    "    print(f\"Configuration {config}: F1 Score = {f1}\")\n",
    "\n",
    "# This section evaluates different configurations of hidden layers to determine the best performing model.\n",
    "# Each configuration is tested by defining the input layers, creating the neural network model, encoding the inputs,\n",
    "# creating the Siamese network, compiling and training the model, and finally calculating the F1 score.\n",
    "# The F1 scores for each configuration are printed and stored in a dictionary for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10. Reporting F1 scores and choosing the best configuration:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Configuration   |   F1 Score |\n",
      "|:----------------|-----------:|\n",
      "| (64, 32, 32)    |   0.483516 |\n",
      "| (256, 128, 128) |   0.461538 |\n",
      "| (128, 64, 64)   |   0.533333 |\n",
      "\n",
      "The best configuration is (128, 64, 64) with an F1 Score of 0.5333.\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with the configurations and their F1 scores\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index', columns=['F1 Score'])  # Convert the results dictionary to a DataFrame\n",
    "results_df.index = results_df.index.map(str)  # Convert tuple indices to strings for better readability\n",
    "results_df = results_df.reset_index()  # Reset the index to move configurations from index to a column\n",
    "results_df.columns = ['Configuration', 'F1 Score']  # Rename columns for clarity\n",
    "\n",
    "# Convert the DataFrame to a markdown table and print it\n",
    "print(results_df.to_markdown(index=False))  # Convert the DataFrame to markdown format and print it without the index\n",
    "\n",
    "# Determine the best configuration based on the highest F1 Score\n",
    "best_config = results_df.loc[results_df['F1 Score'].idxmax()]\n",
    "\n",
    "# Print the best configuration and its F1 score\n",
    "print(f\"\\nThe best configuration is {best_config['Configuration']} with an F1 Score of {best_config['F1 Score']:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commenting on the Results: F1 Score of 0.5333\n",
    "\n",
    "The best hidden layer configuration for our Siamese neural network is (128, 64, 64), with an F1 Score of 0.5333. Here are some key insights:\n",
    "\n",
    "### Performance Evaluation:\n",
    "- **Moderate Performance:** An F1 Score of 0.5333 indicates that the model has a balanced precision and recall. While it is reasonably effective at identifying relevant sentences, there is room for improvement. This score suggests that the model is making a fair number of correct positive identifications but is also missing some relevant sentences (false negatives) or incorrectly identifying irrelevant ones (false positives).\n",
    "\n",
    "### Optimal Configuration:\n",
    "- **Intermediate Layer Sizes:** The configuration (128, 64, 64) outperformed both smaller (64, 32, 32) and larger (256, 128, 128) configurations. This highlights that an intermediate size is better suited to capture the nuances and complexity of the data. Smaller configurations might lack the capacity to learn complex patterns, while larger configurations might overfit the data, capturing noise rather than the underlying structure.\n",
    "- **Balanced Model Complexity:** The chosen configuration balances model complexity and generalization. It is complex enough to capture important features but not so large that it overfits the training data. This balance is crucial in tasks involving query-focused summarization where the model needs to generalize well to unseen questions and answers.\n",
    "\n",
    "### Conclusion:\n",
    "Overall, the F1 Score of 0.5333 reflects a solid starting point for a Siamese neural network tackling the task of query-focused summarization in the medical domain. The results suggest that intermediate hidden layer sizes are most effective, providing a balance between learning capacity and generalization. Further tuning and experimentation could help improve the model's performance, making it more reliable for practical applications in medical question answering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0NeK3gM0dC9"
   },
   "source": [
    "# Task 2: Recurrent NN\n",
    "\n",
    "Implement a more complex Siamese neural network that is composed of the following layers:\n",
    "\n",
    "* An embedding layer that generates embedding vectors of the sentence text with 35 dimensions.\n",
    "* A LSTM layer. You need to determine the size of this LSTM layer, and the text length limit (if needed).\n",
    "* 3 hidden layers and a relu activation function. You need to determine the size of the hidden layers.\n",
    "\n",
    "Train the model with the training data, use the `dev_test` set to determine a good size of the LSTM layer and an appropriate length limit (if needed), and report the final results using the test set. Again, remember to use the test set only after you have determined the optimal parameters of the LSTM layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Import necessary libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\46248722\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Layer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import metrics, optimizers, callbacks\n",
    "from sklearn.metrics import f1_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Set Random Seeds and Define Constants:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MAX_LEN = 300  # Text length limit\n",
    "EPOCHS = 9  # Number of training epochs\n",
    "BATCH_SIZE = 40  # Number of samples per gradient update\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Generate random triplets from CSV file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate random triplets from CSV file\n",
    "def generate_triplets(csv_file):\n",
    "    \"\"\"\n",
    "    Generate random triplets from the given CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    csv_file (str): Path to the CSV file containing the dataset.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Three lists containing anchor inputs, positive inputs, and negative inputs.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(csv_file)\n",
    "    positive_data = data[data[\"label\"] == 1]\n",
    "    negative_data = data[data[\"label\"] == 0]\n",
    "    \n",
    "    a_input = np.array(positive_data[\"question\"].to_list())\n",
    "    p_input = np.array(positive_data[\"sentence text\"].to_list())\n",
    "    n_input = np.array(negative_data[\"sentence text\"].to_list())\n",
    "    \n",
    "    random.seed(10)\n",
    "    indices = list(range(len(a_input)))\n",
    "    random.shuffle(indices)\n",
    "    a_input = a_input[indices].tolist()\n",
    "    p_input = p_input[indices].tolist()\n",
    "    \n",
    "    random.seed(1234)\n",
    "    indices = list(range(len(n_input)))\n",
    "    random.shuffle(indices)\n",
    "    n_input = n_input[indices].tolist()\n",
    "    \n",
    "    return a_input, p_input, n_input[:len(a_input)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Load and transform data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and transform data\n",
    "a_input, p_input, n_input = generate_triplets('training.csv')\n",
    "a_input_test, p_input_test, n_input_test = generate_triplets('dev_test.csv')\n",
    "a_input_test2, p_input_test2, n_input_test2 = generate_triplets('test.csv')\n",
    "\n",
    "# Convert text data to tf-idf vectors\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", max_features=MAX_LEN)\n",
    "tfidf.fit(a_input + p_input + n_input)\n",
    "\n",
    "a_input = tfidf.transform(a_input).toarray()\n",
    "p_input = tfidf.transform(p_input).toarray()\n",
    "n_input = tfidf.transform(n_input).toarray()\n",
    "a_input_test = tfidf.transform(a_input_test).toarray()\n",
    "p_input_test = tfidf.transform(p_input_test).toarray()\n",
    "n_input_test = tfidf.transform(n_input_test).toarray()\n",
    "a_input_test2 = tfidf.transform(a_input_test2).toarray()\n",
    "p_input_test2 = tfidf.transform(p_input_test2).toarray()\n",
    "n_input_test2 = tfidf.transform(n_input_test2).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Define custom distance layer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom distance layer\n",
    "class CustomDistanceLayer(Layer):\n",
    "    \"\"\"\n",
    "    Custom Keras layer to calculate the squared Euclidean distance between inputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)  # Calculate squared Euclidean distance between anchor and positive sample\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)  # Calculate squared Euclidean distance between anchor and negative sample\n",
    "        return ap_distance, an_distance  # Return both distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Define the LSTM Siamese model class:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM Siamese model class\n",
    "class LSTMSiameseModel(Model):\n",
    "    \"\"\"\n",
    "    Custom Keras Model class for a Siamese network with LSTM layers.\n",
    "\n",
    "    This class defines a custom model for a Siamese network using LSTM layers\n",
    "    and a triplet loss function. It includes methods for training, testing,\n",
    "    and calculating the triplet loss, as well as tracking the loss metric\n",
    "    during training and testing.\n",
    "\n",
    "    Attributes:\n",
    "        siamese_network (Model): The Siamese neural network model.\n",
    "        margin (float): Margin for the triplet loss function to ensure a minimum distance \n",
    "                        between positive and negative pairs.\n",
    "        loss_tracker (metrics.Mean): Metric to track the loss during training and testing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, siamese_network, margin=1e-15):\n",
    "        \"\"\"\n",
    "        Initialize the LSTMSiameseModel.\n",
    "\n",
    "        Parameters:\n",
    "            siamese_network (Model): The Siamese neural network model.\n",
    "            margin (float, optional): Margin for the triplet loss function. Default is 1e-15.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "        self.margin = margin\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the Siamese network.\n",
    "\n",
    "        Parameters:\n",
    "            inputs (list): List of input tensors for the Siamese network.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Distances between anchor-positive and anchor-negative samples.\n",
    "        \"\"\"\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        \"\"\"\n",
    "        Custom training step function.\n",
    "\n",
    "        This function performs a single training step, including calculating\n",
    "        the triplet loss, computing gradients, and updating the model weights.\n",
    "\n",
    "        Parameters:\n",
    "            data (tuple): Input data for training, typically a tuple containing\n",
    "                          anchor, positive, and negative samples.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing the current loss value.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.triplet_loss(data)\n",
    "            trainable_vars = self.siamese_network.trainable_weights\n",
    "            gradients = tape.gradient(loss, trainable_vars)\n",
    "            self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "            self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        \"\"\"\n",
    "        Custom testing step function.\n",
    "\n",
    "        This function performs a single testing step, including calculating\n",
    "        the triplet loss and updating the loss tracker.\n",
    "\n",
    "        Parameters:\n",
    "            data (tuple): Input data for testing, typically a tuple containing\n",
    "                          anchor, positive, and negative samples.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing the current loss value.\n",
    "        \"\"\"\n",
    "        loss = self.triplet_loss(data)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    @tf.function\n",
    "    def triplet_loss(self, data):\n",
    "        \"\"\"\n",
    "        Triplet loss function.\n",
    "\n",
    "        This function calculates the triplet loss, which ensures that the\n",
    "        distance between anchor and positive samples is smaller than the\n",
    "        distance between anchor and negative samples by at least the margin.\n",
    "\n",
    "        Parameters:\n",
    "            data (tuple): Input data containing anchor, positive, and negative samples.\n",
    "\n",
    "        Returns:\n",
    "            tensor: Calculated triplet loss.\n",
    "        \"\"\"\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "        loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)\n",
    "        return tf.reduce_sum(loss)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        \"\"\"\n",
    "        Return the metrics to track.\n",
    "\n",
    "        Returns:\n",
    "            list: List of metrics to track during training and testing.\n",
    "        \"\"\"\n",
    "        return [self.loss_tracker]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Evaluate different LSTM and hidden layer configurations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating LSTM units: 36, hidden layers: (32, 32, 32)\n",
      "WARNING:tensorflow:From C:\\Users\\46248722\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - ETA: 0s - loss: 4.6946e-07WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 20s 119ms/step - loss: 4.6946e-07 - val_loss: 1.4837e-08\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - ETA: 0s - loss: 1.1137e-07WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 15s 104ms/step - loss: 1.1137e-07 - val_loss: 1.3999e-07\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - ETA: 0s - loss: 1.3002e-07WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 18s 124ms/step - loss: 1.3002e-07 - val_loss: 1.3287e-07\n",
      "1/1 [==============================] - 1s 884ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "2/2 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "2/2 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "F1 Score for LSTM-36_Hidden-(32, 32, 32): 0.6391752577319586\n",
      "Evaluating LSTM units: 36, hidden layers: (64, 32, 32)\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - ETA: 0s - loss: 6.5374e-07WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 20s 126ms/step - loss: 6.5374e-07 - val_loss: 2.5074e-08\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - ETA: 0s - loss: 5.1524e-08WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 16s 115ms/step - loss: 5.1524e-08 - val_loss: 3.4561e-08\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - ETA: 0s - loss: 1.5456e-08WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 16s 114ms/step - loss: 1.5456e-08 - val_loss: 2.1238e-09\n",
      "Epoch 4/5\n",
      "143/143 [==============================] - ETA: 0s - loss: 7.9604e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 16s 110ms/step - loss: 7.9604e-09 - val_loss: 3.9781e-09\n",
      "Epoch 5/5\n",
      "143/143 [==============================] - ETA: 0s - loss: 2.4783e-09WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 16s 109ms/step - loss: 2.4783e-09 - val_loss: 1.2404e-09\n",
      "1/1 [==============================] - 1s 725ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "2/2 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "2/2 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "F1 Score for LSTM-36_Hidden-(64, 32, 32): 0.6122448979591836\n",
      "Evaluating LSTM units: 64, hidden layers: (32, 32, 32)\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - ETA: 0s - loss: 2.4341e-07WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 142s 976ms/step - loss: 2.4341e-07 - val_loss: 1.4445e-07\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - ETA: 0s - loss: 5.2642e-08WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 136s 949ms/step - loss: 5.2642e-08 - val_loss: 2.4830e-08\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - ETA: 0s - loss: 2.3943e-07WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 149s 1s/step - loss: 2.3943e-07 - val_loss: 6.1905e-08\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "2/2 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "2/2 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "2/2 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "F1 Score for LSTM-64_Hidden-(32, 32, 32): 0.6326530612244898\n",
      "Evaluating LSTM units: 64, hidden layers: (64, 32, 32)\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - ETA: 0s - loss: 5.1374e-07WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 351s 2s/step - loss: 5.1374e-07 - val_loss: 1.0652e-07\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - ETA: 0s - loss: 1.9473e-07WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 314s 2s/step - loss: 1.9473e-07 - val_loss: 2.6520e-08\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - ETA: 0s - loss: 2.5133e-07WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "143/143 [==============================] - 295s 2s/step - loss: 2.5133e-07 - val_loss: 9.3261e-08\n",
      "1/1 [==============================] - 14s 14s/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 266ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "1/1 [==============================] - 0s 326ms/step\n",
      "2/2 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "1/1 [==============================] - 0s 403ms/step\n",
      "1/1 [==============================] - 0s 450ms/step\n",
      "1/1 [==============================] - 1s 643ms/step\n",
      "1/1 [==============================] - 0s 241ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 1s 634ms/step\n",
      "1/1 [==============================] - 0s 264ms/step\n",
      "2/2 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 240ms/step\n",
      "1/1 [==============================] - 0s 391ms/step\n",
      "1/1 [==============================] - 0s 443ms/step\n",
      "1/1 [==============================] - 1s 597ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 1s 528ms/step\n",
      "1/1 [==============================] - 0s 497ms/step\n",
      "1/1 [==============================] - 0s 414ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 336ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "2/2 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 1s 661ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "F1 Score for LSTM-64_Hidden-(64, 32, 32): 0.6326530612244898\n",
      "All configuration results:\n",
      "Configuration LSTM-36_Hidden-(32, 32, 32): F1 Score = 0.6391752577319586\n",
      "Configuration LSTM-36_Hidden-(64, 32, 32): F1 Score = 0.6122448979591836\n",
      "Configuration LSTM-64_Hidden-(32, 32, 32): F1 Score = 0.6326530612244898\n",
      "Configuration LSTM-64_Hidden-(64, 32, 32): F1 Score = 0.6326530612244898\n"
     ]
    }
   ],
   "source": [
    "# Evaluate different LSTM and hidden layer configurations\n",
    "lstm_units_configs = [36, 64]  # Different configurations for LSTM units\n",
    "hidden_layer_configs = [\n",
    "    (32, 32, 32),  # Try other hidden layers but worse result (128,64,64) and (256,128,128) too much time consuming\n",
    "    (64, 32, 32),\n",
    "]\n",
    "\n",
    "results = {}  # Dictionary to store F1 scores for each configuration\n",
    "\n",
    "for lstm_units in lstm_units_configs:\n",
    "    for hidden_config in hidden_layer_configs:\n",
    "        print(f\"Evaluating LSTM units: {lstm_units}, hidden layers: {hidden_config}\")\n",
    "\n",
    "        # Set random seeds for reproducibility before each configuration\n",
    "        random.seed(1234)\n",
    "        np.random.seed(1234)\n",
    "        tf.random.set_seed(1234)\n",
    "\n",
    "        # Build the Siamese LSTM network\n",
    "        embed_size = 35  # Embedding size\n",
    "        first_sent_in_a = Input(shape=(MAX_LEN,))  # Input layer for anchor sentences\n",
    "        second_sent_in_p = Input(shape=(MAX_LEN,))  # Input layer for positive sentences\n",
    "        third_sent_in_n = Input(shape=(MAX_LEN,))  # Input layer for negative sentences\n",
    "\n",
    "        embedding_layer = Embedding(input_dim=MAX_LEN+1, output_dim=embed_size, input_length=MAX_LEN, trainable=True)  # Embedding layer\n",
    "        simple_lstm = LSTM(units=lstm_units, return_sequences=False)  # LSTM layer\n",
    "\n",
    "        first_sent_encoded_a = simple_lstm(embedding_layer(first_sent_in_a))  # Encoding anchor sentences\n",
    "        second_sent_encoded_p = simple_lstm(embedding_layer(second_sent_in_p))  # Encoding positive sentences\n",
    "        third_sent_encoded_n = simple_lstm(embedding_layer(third_sent_in_n))  # Encoding negative sentences\n",
    "\n",
    "        # Define the neural network model with hidden layers\n",
    "        model = Sequential([\n",
    "            Dense(hidden_config[0], activation='relu'),  # First hidden layer with ReLU activation\n",
    "            Dense(hidden_config[1], activation='relu'),  # Second hidden layer with ReLU activation\n",
    "            Dense(hidden_config[2], activation='relu')   # Third hidden layer with ReLU activation\n",
    "        ])\n",
    "\n",
    "        encoded_1 = model(first_sent_encoded_a)  # Pass anchor encodings through the model\n",
    "        encoded_2 = model(second_sent_encoded_p)  # Pass positive encodings through the model\n",
    "        encoded_3 = model(third_sent_encoded_n)  # Pass negative encodings through the model\n",
    "\n",
    "        # Define the custom distance layer\n",
    "        loss_layer = CustomDistanceLayer()(encoded_1, encoded_2, encoded_3)\n",
    "        siamese_network = Model([first_sent_in_a, second_sent_in_p, third_sent_in_n], loss_layer)\n",
    "\n",
    "        # Compile and train the LSTM Siamese model\n",
    "        lstm_siamese_model = LSTMSiameseModel(siamese_network)\n",
    "        lstm_siamese_model.compile(optimizer=optimizers.Adam(learning_rate=0.001))\n",
    "        callback = callbacks.EarlyStopping(monitor='loss', patience=1)  # Early stopping callback\n",
    "\n",
    "        # Train the model\n",
    "        lstm_siamese_model.fit([a_input, p_input, n_input], epochs=5, batch_size=80,\n",
    "                               validation_data=([a_input_test, p_input_test, n_input_test]), callbacks=[callback])\n",
    "\n",
    "        # Define the summarizer function\n",
    "        def lstm_nn_summarizer(csvfile, questionids, n=1):\n",
    "            test_data = pd.read_csv(csvfile)\n",
    "            total = []\n",
    "            for qid in questionids:\n",
    "                # Extract anchor, positive, and negative samples for the given question ID\n",
    "                anchor_input = list(test_data.loc[(test_data[\"qid\"] == qid) & (test_data[\"sentid\"] == 0)][\"question\"])\n",
    "                positive_input = list(test_data.loc[(test_data[\"qid\"] == qid) & (test_data[\"sentid\"] == 0)][\"sentence text\"])\n",
    "                negative_input = list(test_data.loc[(test_data[\"qid\"] == qid)][\"sentence text\"])\n",
    "\n",
    "                # Repeat the positive and anchor inputs to match the number of negative inputs\n",
    "                positive_input *= len(negative_input)\n",
    "                anchor_input *= len(negative_input)\n",
    "\n",
    "                # Transform the text data into tf-idf vectors\n",
    "                a_input = tfidf.transform(anchor_input).toarray()\n",
    "                p_input = tfidf.transform(positive_input).toarray()\n",
    "                n_input = tfidf.transform(negative_input).toarray()\n",
    "\n",
    "                # Get predictions from the Siamese model\n",
    "                ap_distance_test, an_distance_test = lstm_siamese_model.predict([a_input, p_input, n_input], batch_size=40)\n",
    "                sorted_indices = list(np.argsort(an_distance_test))  # Sort the indices based on distance to get the most similar sentences\n",
    "                total.append(sorted_indices[:n])\n",
    "            return total\n",
    "\n",
    "        # Evaluate the LSTM Siamese model\n",
    "        def calculate_lstm_labels(csvfile, n):\n",
    "            test_data = pd.read_csv(csvfile)\n",
    "            total_predict = []\n",
    "            total_true = []\n",
    "            for qid in list(test_data[\"qid\"].unique())[:n]:\n",
    "                predict_values = lstm_nn_summarizer(csvfile, [qid], len(test_data.loc[(test_data[\"qid\"] == qid)]))[0]\n",
    "                predicted_0 = predict_values[-1]  # Last prediction as 0\n",
    "                predicted_1 = predict_values[0]   # First prediction as 1\n",
    "\n",
    "                y_true = list(test_data.loc[(test_data[\"qid\"] == qid) & (test_data[\"sentid\"] == predicted_1)][\"label\"])[0]\n",
    "                y_false = list(test_data.loc[(test_data[\"qid\"] == qid) & (test_data[\"sentid\"] == predicted_0)][\"label\"])[0]\n",
    "\n",
    "                total_predict.extend([1, 0])  # Extend predicted values\n",
    "                total_true.extend([y_true, y_false])  # Extend true values\n",
    "            return total_predict, total_true\n",
    "\n",
    "        total_predict, total_true = calculate_lstm_labels(\"test.csv\", 50)\n",
    "        f1 = f1_score(total_true, total_predict)  # Calculate F1 score\n",
    "        config_name = f\"LSTM-{lstm_units}_Hidden-{hidden_config}\"  # Configuration name\n",
    "        results[config_name] = f1  # Store F1 score\n",
    "        print(f\"F1 Score for {config_name}: {f1}\")\n",
    "\n",
    "# Print all results\n",
    "print(\"All configuration results:\")\n",
    "for config, f1 in results.items():\n",
    "    print(f\"Configuration {config}: F1 Score = {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Display and summarize results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Configuration               |   F1 Score |\n",
      "|:----------------------------|-----------:|\n",
      "| LSTM-36_Hidden-(32, 32, 32) |   0.639175 |\n",
      "| LSTM-36_Hidden-(64, 32, 32) |   0.612245 |\n",
      "| LSTM-64_Hidden-(32, 32, 32) |   0.632653 |\n",
      "| LSTM-64_Hidden-(64, 32, 32) |   0.632653 |\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with the configurations and their F1 scores\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index', columns=['F1 Score'])  # Convert results dictionary to DataFrame\n",
    "results_df.index = results_df.index.map(str)  # Convert tuple indices to string for better readability\n",
    "results_df = results_df.reset_index()  # Reset index to get configuration as a column\n",
    "results_df.columns = ['Configuration', 'F1 Score']  # Rename columns\n",
    "\n",
    "# Convert the DataFrame to a markdown table\n",
    "print(results_df.to_markdown(index=False))  # Print DataFrame as markdown table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commenting on the Results:\n",
    "\n",
    "- LSTM-36_Hidden-(32, 32, 32): Achieved the highest F1 score of 0.639175 with 36 LSTM units and three hidden layers, each with 32 units.\n",
    "- LSTM-36_Hidden-(64, 32, 32): Slightly lower F1 score of 0.612245 with 36 LSTM units and a more complex hidden layer structure.\n",
    "- LSTM-64_Hidden-(32, 32, 32): F1 score of 0.632653 with 64 LSTM units and three hidden layers of 32 units each.\n",
    "- LSTM-64_Hidden-(64, 32, 32): Similar F1 score of 0.632653 with 64 LSTM units and a more complex hidden layer structure.\n",
    "   \n",
    "   \n",
    "   \n",
    "- LSTM Units: Increasing LSTM units from 36 to 64 has minimal impact on performance.\n",
    "- Hidden Layer Complexity: A more complex hidden layer structure does not necessarily lead to better performance. The best results were achieved with simpler hidden layers (32, 32, 32).\n",
    "\n",
    "### Comparison with Simple Siamese NN\n",
    "- 0.639175 > 0.533\n",
    "- The LSTM-based Siamese network outperforms the Simple Siamese NN (without LSTM layers) by effectively capturing sequential dependencies in the input data. This highlights the importance of sequence modeling in improving performance for tasks involving text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRTpswvZ1Zlr"
   },
   "source": [
    "# Task 3: Transformer\n",
    "\n",
    "Implement a simple Transformer neural network that is composed of the following layers:\n",
    "\n",
    "* Use BERT as feature extractor for each token.\n",
    "* A few of transformer encoder layers, hidden dimension 768. You need to determine how many layers to use between 1~3.\n",
    "* A few of transformer decoder layers, hidden dimension 768. You need to determine how many layers to use between 1~3.\n",
    "* 1 hidden layer with size 512.\n",
    "* The final output layer with one cell for binary classification to predict whether two inputs are related or not.\n",
    "\n",
    "Note that each input for this model should be a concatenation of a positive pair (i.e. question + one answer) or a negative pair (i.e. question + not related sentence). The format is usually like [CLS]+ question + [SEP] + a positive/negative sentence.\n",
    "\n",
    "Train the model with the training data, use the dev_test set to determine a good size of the transformer layers, and report the final results using the test set. Again, remember to use the test set only after you have determined the optimal parameters of the transformer layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Import necessary libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Data preparation functions:**\n",
    "- **Function: 'load_dataset'**\n",
    "- This function loads and preprocesses the dataset from a CSV file. It converts questions and answers into BERT-compatible format by adding special tokens ([CLS] and [SEP]).\n",
    "\n",
    "\n",
    "- **Function: 'tokenize_pairs'**\n",
    "- This function tokenizes the input pairs using the BERT tokenizer, ensuring they are in the correct format for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(csvfile, num_samples):\n",
    "    \"\"\"\n",
    "    Load and preprocess dataset from a CSV file.\n",
    "    \n",
    "    This function reads a CSV file containing question-answer pairs, extracts the necessary columns,\n",
    "    adds special tokens required by BERT, and prepares the data for model input.\n",
    "    \n",
    "    Parameters:\n",
    "    csvfile (str): Path to the CSV file containing the dataset.\n",
    "    num_samples (int): Number of samples to load from the dataset. Random sampling is applied to limit the number of samples.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: \n",
    "        - concatenated_pairs (list): A list of strings, each string being a concatenation of a question and an answer, formatted for BERT input.\n",
    "        - labels (list): A list of labels corresponding to each question-answer pair, indicating whether the answer is relevant (1) or not (0).\n",
    "    \"\"\"\n",
    "    # Load data from CSV and randomly sample the specified number of rows\n",
    "    data = pd.read_csv(csvfile).sample(num_samples)\n",
    "    \n",
    "    # Extract question and answer columns as lists\n",
    "    question_input = data[\"question\"].to_list()\n",
    "    answers_input = data[\"sentence text\"].to_list()\n",
    "    \n",
    "    # Add special BERT tokens to each question and answer\n",
    "    question_encoded = [\"[CLS] \" + x + \" [SEP] \" for x in question_input]\n",
    "    answers_encoded = [x + \" [SEP]\" for x in answers_input]\n",
    "    \n",
    "    # Concatenate each question with its corresponding answer\n",
    "    concatenated_pairs = [q + a for q, a in zip(question_encoded, answers_encoded)]\n",
    "    \n",
    "    # Extract labels as a list\n",
    "    labels = data[\"label\"].to_list()\n",
    "    \n",
    "    # Return the concatenated question-answer pairs and labels\n",
    "    return concatenated_pairs, labels\n",
    "\n",
    "def tokenize_pairs(pairs, max_length):\n",
    "    \"\"\"\n",
    "    Tokenize input pairs using BERT tokenizer.\n",
    "    \n",
    "    This function takes a list of concatenated question-answer pairs and tokenizes them using the BERT tokenizer.\n",
    "    It converts the pairs into tensors suitable for model input, handling padding and truncation to ensure uniform length.\n",
    "    \n",
    "    Parameters:\n",
    "    pairs (list): A list of strings, where each string is a concatenation of a question and an answer, formatted for BERT input.\n",
    "    max_length (int): Maximum length for tokenization. Pairs longer than this length will be truncated, and shorter pairs will be padded.\n",
    "    \n",
    "    Returns:\n",
    "    dict: \n",
    "        - 'input_ids': Tensor of tokenized input IDs.\n",
    "        - 'attention_mask': Tensor of attention masks.\n",
    "        - Other token type IDs if applicable.\n",
    "    \"\"\"\n",
    "    # Initialize the BERT tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Tokenize the input pairs, with padding and truncation to the specified maximum length\n",
    "    tokenized_output = tokenizer(pairs, return_tensors='tf', max_length=max_length, padding=True, truncation=True)\n",
    "    \n",
    "    # Return the tokenized output, which includes 'input_ids', 'attention_mask', and possibly other fields\n",
    "    return tokenized_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Model definition:**\n",
    "- **Class: 'CustomTransformerEncoder'**\n",
    "- This class defines a transformer encoder layer, which includes multi-head attention and feed-forward neural networks with layer normalization.\n",
    "\n",
    "\n",
    "- **Class: 'CustomTransformerDecoder'**\n",
    "- This class defines a transformer decoder layer, which also includes multi-head attention and feed-forward neural networks with layer normalization, but it attends to both its inputs and the encoder's outputs.\n",
    "\n",
    "\n",
    "- **Class: 'CustomTransformerModel'**\n",
    "- This class defines the final transformer model using the encoder and decoder layers. It uses BERT as a feature extractor and then applies the custom transformer layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim):\n",
    "        \"\"\"\n",
    "        Initialize the Transformer Encoder Layer.\n",
    "        \n",
    "        This layer applies multi-head attention followed by a feed-forward network. \n",
    "        Layer normalization is applied after both the attention and feed-forward steps.\n",
    "        \n",
    "        Parameters:\n",
    "        hidden_dim (int): Dimension of the hidden layers.\n",
    "        num_heads (int): Number of attention heads. This defines how many attention mechanisms run in parallel.\n",
    "        ff_dim (int): Dimension of the feed-forward layer. This is the inner layer dimension before projecting back to hidden_dim.\n",
    "        \"\"\"\n",
    "        super(CustomTransformerEncoder, self).__init__()\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation='relu'),  # Inner feed-forward layer\n",
    "            tf.keras.layers.Dense(hidden_dim),  # Project back to hidden_dim\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder layer.\n",
    "        \n",
    "        This method applies multi-head attention followed by a feed-forward network, \n",
    "        with layer normalization applied after each step.\n",
    "        \n",
    "        Parameters:\n",
    "        inputs (Tensor): Input tensor of shape (batch_size, sequence_length, hidden_dim).\n",
    "        \n",
    "        Returns:\n",
    "        Tensor: Output tensor after attention and feed-forward network, of the same shape as inputs.\n",
    "        \n",
    "        Example:\n",
    "        >>> encoder_layer = CustomTransformerEncoder(hidden_dim=768, num_heads=8, ff_dim=2048)\n",
    "        >>> outputs = encoder_layer(inputs)\n",
    "        \"\"\"\n",
    "        attn_output = self.attention(inputs, inputs)  # Self-attention\n",
    "        out1 = self.layernorm1(inputs + attn_output)  # Add & Norm\n",
    "        ffn_output = self.ffn(out1)  # Feed-forward\n",
    "        return self.layernorm2(out1 + ffn_output)  # Add & Norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformerDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim):\n",
    "        \"\"\"\n",
    "        Initialize the Transformer Decoder Layer.\n",
    "        \n",
    "        This layer applies self-attention, cross-attention (attending to encoder outputs), \n",
    "        followed by a feed-forward network. Layer normalization is applied after each step.\n",
    "        \n",
    "        Parameters:\n",
    "        hidden_dim (int): Dimension of the hidden layers.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        ff_dim (int): Dimension of the feed-forward layer.\n",
    "        \"\"\"\n",
    "        super(CustomTransformerDecoder, self).__init__()\n",
    "        self.attention1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)\n",
    "        self.attention2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(hidden_dim),\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, enc_outputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder layer.\n",
    "        \n",
    "        This method applies self-attention, cross-attention (with encoder outputs), \n",
    "        followed by a feed-forward network, with layer normalization applied after each step.\n",
    "        \n",
    "        Parameters:\n",
    "        inputs (Tensor): Input tensor of shape (batch_size, sequence_length, hidden_dim).\n",
    "        enc_outputs (Tensor): Encoder output tensor to attend to, of shape (batch_size, sequence_length, hidden_dim).\n",
    "        \n",
    "        Returns:\n",
    "        Tensor: Output tensor after attention and feed-forward network, of the same shape as inputs.\n",
    "        \n",
    "        Example:\n",
    "        >>> decoder_layer = CustomTransformerDecoder(hidden_dim=768, num_heads=8, ff_dim=2048)\n",
    "        >>> outputs = decoder_layer(inputs, enc_outputs)\n",
    "        \"\"\"\n",
    "        attn1 = self.attention1(inputs, inputs)  # Self-attention\n",
    "        out1 = self.layernorm1(inputs + attn1)  # Add & Norm\n",
    "        attn2 = self.attention2(out1, enc_outputs)  # Cross-attention with encoder outputs\n",
    "        out2 = self.layernorm2(out1 + attn2)  # Add & Norm\n",
    "        ffn_output = self.ffn(out2)  # Feed-forward\n",
    "        return self.layernorm3(out2 + ffn_output)  # Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformerModel(tf.keras.Model):\n",
    "    def __init__(self, num_encoder_layers, num_decoder_layers, hidden_dim, ff_dim, hidden_layer_size, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the Transformer Model.\n",
    "        \n",
    "        This model integrates a pre-trained BERT model as the initial embedding layer, \n",
    "        followed by multiple transformer encoder and decoder layers, and a final classification layer.\n",
    "        \n",
    "        Parameters:\n",
    "        num_encoder_layers (int): Number of encoder layers.\n",
    "        num_decoder_layers (int): Number of decoder layers.\n",
    "        hidden_dim (int): Dimension of the hidden layers.\n",
    "        ff_dim (int): Dimension of the feed-forward layer.\n",
    "        hidden_layer_size (int): Size of the hidden layer before the output layer.\n",
    "        \"\"\"\n",
    "        super(CustomTransformerModel, self).__init__(**kwargs)\n",
    "        self.bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "        self.encoder_layers = [CustomTransformerEncoder(hidden_dim, 8, ff_dim) for _ in range(num_encoder_layers)]\n",
    "        self.decoder_layers = [CustomTransformerDecoder(hidden_dim, 8, ff_dim) for _ in range(num_decoder_layers)]\n",
    "        self.hidden_layer = tf.keras.layers.Dense(hidden_layer_size, activation='relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer model.\n",
    "        \n",
    "        This method processes the inputs through the BERT model for initial embedding, \n",
    "        followed by multiple transformer encoder and decoder layers, and a final classification layer.\n",
    "        \n",
    "        Parameters:\n",
    "        inputs (Tensor): Input tensor of shape (batch_size, sequence_length, hidden_dim).\n",
    "        training (bool): Whether the model is in training mode.\n",
    "        \n",
    "        Returns:\n",
    "        Tensor: Output tensor after passing through the model, of shape (batch_size, 1).\n",
    "        \n",
    "        \"\"\"\n",
    "        bert_output = self.bert(inputs)[0]  # Get BERT embeddings\n",
    "        x = bert_output\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x, training=training)  # Pass through encoder layers\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            x = decoder_layer(x, bert_output, training=training)  # Pass through decoder layers\n",
    "        x = self.hidden_layer(x[:, 0, :])  # Take the first token's output and pass through hidden layer\n",
    "        return self.output_layer(x)  # Final classification layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Model training and evaluation:**\n",
    "- **Function: 'train_transformer_model'**\n",
    "- This function trains the transformer model on the training dataset and validates it on the development/test dataset.\n",
    "\n",
    "- **Function: 'evaluate_transformer_model'**\n",
    "- This function evaluates the transformer model on the test dataset and calculates precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer_model(model, train_inputs, train_labels, dev_test_inputs, dev_test_labels, batch_size=32, epochs=3):\n",
    "    \"\"\"\n",
    "    Train the transformer model.\n",
    "    \n",
    "    Parameters:\n",
    "    model (tf.keras.Model): The transformer model to train.\n",
    "    train_inputs (dict): Tokenized training inputs.\n",
    "        - The dictionary contains:\n",
    "            'input_ids': Tensor of shape (batch_size, sequence_length) with token ids for each input.\n",
    "    train_labels (list): Training labels.\n",
    "        - A list of binary labels (0 or 1) for each training example.\n",
    "    dev_test_inputs (dict): Tokenized development/test inputs.\n",
    "        - The dictionary contains:\n",
    "            'input_ids': Tensor of shape (batch_size, sequence_length) with token ids for each input.\n",
    "    dev_test_labels (list): Development/test labels.\n",
    "        - A list of binary labels (0 or 1) for each development/test example.\n",
    "    batch_size (int): Batch size for training.\n",
    "        - Number of samples per gradient update.\n",
    "    epochs (int): Number of epochs to train the model.\n",
    "        - Number of complete passes through the training dataset.\n",
    "    \n",
    "    Returns:\n",
    "    tf.keras.callbacks.History: Training history.\n",
    "        - A record of training loss values and metrics values at successive epochs.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Create TensorFlow datasets from input tensors and labels\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs['input_ids'], train_labels)).batch(batch_size)\n",
    "    dev_test_dataset = tf.data.Dataset.from_tensor_slices((dev_test_inputs['input_ids'], dev_test_labels)).batch(batch_size)\n",
    "    \n",
    "    # Train the model on the training dataset and validate on the dev/test dataset\n",
    "    history = model.fit(train_dataset, validation_data=dev_test_dataset, epochs=epochs)\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_transformer_model(model, test_inputs, test_labels, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate the transformer model.\n",
    "    \n",
    "    Parameters:\n",
    "    model (tf.keras.Model): The transformer model to evaluate.\n",
    "    test_inputs (dict): Tokenized test inputs.\n",
    "        - The dictionary contains:\n",
    "            'input_ids': Tensor of shape (batch_size, sequence_length) with token ids for each input.\n",
    "    test_labels (list): Test labels.\n",
    "        - A list of binary labels (0 or 1) for each test example.\n",
    "    threshold (float): Threshold for binary classification.\n",
    "        - Probability threshold to classify the output as 1 (related) or 0 (not related). Default is 0.5.\n",
    "    \n",
    "    Returns:\n",
    "    dict: Precision, recall, and F1 score.\n",
    "        - 'precision': Precision score for the test set.\n",
    "        - 'recall': Recall score for the test set.\n",
    "        - 'f1': F1 score for the test set.\n",
    "    \"\"\"\n",
    "    # Predict the probabilities for the test inputs\n",
    "    test_predictions = model.predict(test_inputs['input_ids']).flatten()\n",
    "    \n",
    "    # Apply the threshold to get binary predictions\n",
    "    test_predictions = (test_predictions > threshold).astype(int)\n",
    "    \n",
    "    # Convert the test labels to a numpy array for compatibility with sklearn metrics\n",
    "    y_true = np.array(test_labels)\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(y_true, test_predictions)\n",
    "    recall = recall_score(y_true, test_predictions)\n",
    "    f1 = f1_score(y_true, test_predictions)\n",
    "    \n",
    "    return {'precision': precision, 'recall': recall, 'f1': f1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Summarization function:**\n",
    "- Function: 'summarize_answers'\n",
    "- This function uses the trained model to predict and return the IDs of the top n sentences with the highest prediction scores for given question IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_answers(model, csvfile, questionids, n=1):\n",
    "    \"\"\"\n",
    "    Return the IDs of the top `n` sentences with the highest predicted scores.\n",
    "    \n",
    "    Parameters:\n",
    "    model (tf.keras.Model): The trained transformer model.\n",
    "        - The model that has been trained on similar data and is used to predict the relevance of answer sentences.\n",
    "    csvfile (str): Path to the CSV file containing the dataset.\n",
    "        - The file should contain columns 'qid' for question IDs, 'question' for question text, and 'sentence text' for the candidate answers.\n",
    "    questionids (list): List of question IDs.\n",
    "        - A list of question IDs for which the top `n` answers are to be summarized.\n",
    "    n (int): Number of top sentences to return.\n",
    "        - The number of top-scoring sentences to be returned for each question ID.\n",
    "    \n",
    "    Returns:\n",
    "    list: List of lists containing sentence IDs.\n",
    "        - Each element in the list corresponds to a question ID from the input list, containing the IDs of the top `n` sentences with the highest predicted scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the dataset from the specified CSV file\n",
    "    data = pd.read_csv(csvfile)\n",
    "    \n",
    "    # Initialize a list to store the results for each question ID\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over each question ID provided\n",
    "    for qid in questionids:\n",
    "        # Filter the dataset to get the subset of rows corresponding to the current question ID\n",
    "        subset = data[data['qid'] == qid]\n",
    "        \n",
    "        # Extract the list of questions and corresponding answers from the subset\n",
    "        questions = subset['question'].tolist()\n",
    "        answers = subset['sentence text'].tolist()\n",
    "        \n",
    "        # Concatenate questions and answers into pairs with special tokens [CLS] and [SEP]\n",
    "        pairs = [\"[CLS] \" + q + \" [SEP] \" + a + \" [SEP]\" for q, a in zip(questions, answers)]\n",
    "        \n",
    "        # Tokenize the pairs using the BERT tokenizer\n",
    "        tokenized = tokenize_pairs(pairs, MAX_LEN)\n",
    "        \n",
    "        # Predict the relevance scores for each pair using the trained model\n",
    "        predictions = model.predict(tokenized['input_ids']).flatten()\n",
    "        \n",
    "        # Get the indices of the top `n` sentences with the highest predicted scores\n",
    "        top_indices = np.argsort(predictions)[-n:]\n",
    "        \n",
    "        # Append the indices to the results list\n",
    "        results.append(top_indices.tolist())\n",
    "    \n",
    "    # Return the list of results containing top `n` sentence IDs for each question ID\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Loading data, tokenizing, defining the model, training, evaluating, and summarizing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From C:\\Users\\46248722\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\46248722\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:From C:\\Users\\46248722\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "32/32 [==============================] - 900s 27s/step - loss: 0.6984 - accuracy: 0.6600 - val_loss: 0.5948 - val_accuracy: 0.7800\n",
      "Epoch 2/3\n",
      "32/32 [==============================] - 838s 26s/step - loss: 0.6182 - accuracy: 0.6910 - val_loss: 0.6011 - val_accuracy: 0.7300\n",
      "Epoch 3/3\n",
      "32/32 [==============================] - 847s 26s/step - loss: 0.5846 - accuracy: 0.6980 - val_loss: 0.6216 - val_accuracy: 0.6200\n",
      "4/4 [==============================] - 37s 6s/step\n",
      "Precision: 0.2727\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.4286\n",
      "1/1 [==============================] - 14s 14s/step\n",
      "1/1 [==============================] - 1s 892ms/step\n",
      "[[20, 28, 25, 18], [0, 1, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare datasets\n",
    "train_pairs, train_labels = load_dataset('training.csv', 1000)\n",
    "dev_test_pairs, dev_test_labels = load_dataset('dev_test.csv', 100)\n",
    "test_pairs, test_labels = load_dataset('test.csv', 100)\n",
    "\n",
    "# Tokenize pairs\n",
    "MAX_LEN = 128\n",
    "train_inputs = tokenize_pairs(train_pairs, MAX_LEN)\n",
    "dev_test_inputs = tokenize_pairs(dev_test_pairs, MAX_LEN)\n",
    "test_inputs = tokenize_pairs(test_pairs, MAX_LEN)\n",
    "\n",
    "# Define model parameters\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "hidden_dim = 768\n",
    "ff_dim = 2048\n",
    "hidden_layer_size = 512\n",
    "\n",
    "# Instantiate and compile the model\n",
    "transformer_model = CustomTransformerModel(num_encoder_layers, num_decoder_layers, hidden_dim, ff_dim, hidden_layer_size)\n",
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "train_transformer_model(transformer_model, train_inputs, train_labels, dev_test_inputs, dev_test_labels, batch_size=32, epochs=3)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = evaluate_transformer_model(transformer_model, test_inputs, test_labels, threshold=0.19)\n",
    "print(f\"Precision: {evaluation_results['precision']:.4f}\")\n",
    "print(f\"Recall: {evaluation_results['recall']:.4f}\")\n",
    "print(f\"F1 Score: {evaluation_results['f1']:.4f}\")\n",
    "\n",
    "# Test the summarizer function\n",
    "print(summarize_answers(transformer_model, 'test.csv', [6, 4220], n=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The summarizer function successfully returns the IDs** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Transformer Model Performance\n",
    "\n",
    "**Precision:** 0.2727  \n",
    "**Recall:** 1.0000  \n",
    "**F1 Score:** 0.4286  \n",
    "\n",
    "The Transformer model achieved a recall of 1.0000, indicating that it successfully identified all relevant sentences (true positives) but at the cost of low precision (0.2727), meaning many irrelevant sentences were also incorrectly identified as relevant (false positives). The resulting F1 score of 0.4286 reflects this imbalance.\n",
    "\n",
    "If I have more time in this assignment and the university's PC has more capicity, I will do the following steps to improve the performance:\n",
    "\n",
    "1. **Extended training:** Allowing more epochs for the Transformer model to learn and adapt could improve performance.\n",
    "2. **Threshold tuning:** Systematically experimenting with different classification thresholds could find an optimal balance between precision and recall.\n",
    "3. **Model tuning:** Adjusting hyperparameters, such as the number of encoder layers, hidden dimensions, and learning rate, might enhance model performance.\n",
    "4. **Data augmentation:** Increasing the amount of training data or employing data augmentation techniques could help the Transformer model generalize better.\n",
    "\n",
    "Overall, while the Transformer model shows potential, it requires further tuning and training to match or surpass the performance of LSTM-based models in this task."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "a7b63e7410c98f344f02082f10d790581d1dba1eeb1c8fe30f342f6109f0429e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
